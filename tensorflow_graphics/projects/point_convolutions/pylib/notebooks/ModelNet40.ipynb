{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelNet40_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2xKG5Pgu_SX",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RkUBOZbu3iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31faWJ7pCNGR"
      },
      "source": [
        "# Point Clouds for tensorflow_graphics\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/schellmi42/tensorflow_graphics_point_clouds/blob/master/pylib/notebooks/ModelNet40.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/schellmi42/tensorflow_graphics_point_clouds/blob/master/pylib/notebooks/ModelNet40.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo1bELqqRAKF",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqSeyzzZQUDV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Clone repositories, install requirements and custom_op package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vtvcRP3QtTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone repositories\n",
        "!rm -r graphics\n",
        "!git clone https://github.com/schellmi42/graphics\n",
        "\n",
        "# install requirements and load tfg module \n",
        "!pip install -r graphics/requirements.txt\n",
        "\n",
        "# install custom ops\n",
        "!pip install graphics/tensorflow_graphics/projects/point_convolutions/custom_ops/pkg_builds/tf_2.2.0/*.whl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D-q8bUu7TcJ",
        "colab_type": "text"
      },
      "source": [
        "### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lTxNysx7Lfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# (this is equivalent to export PYTHONPATH='$HOME/graphics:/content/graphics:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics\")\n",
        "\n",
        "# load point cloud module \n",
        "# (this is equivalent to export PYTHONPATH='/content/graphics/tensorflow_graphics/projects/point_convolutions:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics/tensorflow_graphics/projects/point_convolutions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gUTDsKERxWh",
        "colab_type": "text"
      },
      "source": [
        "Check if it loads without errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_49mMLOSOX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_graphics as tfg\n",
        "import pylib.pc as pc\n",
        "import numpy as np\n",
        "\n",
        "print('TensorFlow version: %s'%tf.__version__)\n",
        "print('TensorFlow-Graphics version: %s'%tfg.__version__)\n",
        "print('Point Cloud Module: ', pc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_XXAVBau-gg",
        "colab_type": "text"
      },
      "source": [
        "## Classification on ModelNet40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQV3Ykr16m8",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIXyqlmBv5q1",
        "colab_type": "text"
      },
      "source": [
        "First we load the data consisting of 10k points per model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW4GZdArv-3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip \n",
        "!echo '---unzipping---'\n",
        "!unzip -q modelnet40_normal_resampled.zip \n",
        "!echo '[done]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd2ElOdHwBnW",
        "colab_type": "text"
      },
      "source": [
        "Next we load the data, using the input function in the `io` module.\n",
        "\n",
        "To speed up this tutorial, we only load a subset of the points per model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnt1Byd4sWHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import pylib.pc as pc\n",
        "import pylib.io as io\n",
        "import numpy as np\n",
        "import tensorflow_graphics\n",
        "import os, time\n",
        "\n",
        "\n",
        "quick_test = False  # only load 100 models\n",
        "\n",
        "# -- loading data ---\n",
        "\n",
        "data_dir = 'modelnet40_normal_resampled/'\n",
        "num_classes = 40  # modelnet 10 or 40\n",
        "points_per_file = 5000  # number of points loaded per model\n",
        "\n",
        "# load category names\n",
        "category_names = []\n",
        "with open(data_dir + f'modelnet{num_classes}_shape_names.txt') as inFile:\n",
        "  for line in inFile:\n",
        "    category_names.append(line.replace('\\n', ''))\n",
        "\n",
        "# load names of training files\n",
        "train_set = []\n",
        "train_labels = []\n",
        "with open(data_dir + f'modelnet{num_classes}_train.txt') as inFile:\n",
        "  for line in inFile:\n",
        "    line = line.replace('\\n', '')\n",
        "    category = line[:-5]\n",
        "    train_set.append(data_dir + category + '/' + line + '.txt')\n",
        "    if category not in category_names:\n",
        "      raise ValueError('Unknown category ' + category)\n",
        "    train_labels.append(category_names.index(category))\n",
        "\n",
        "# load names of test files\n",
        "test_set = []\n",
        "test_labels = []\n",
        "with open(data_dir + f'modelnet{num_classes}_test.txt') as inFile:\n",
        "  for line in inFile:\n",
        "    line = line.replace('\\n', '')\n",
        "    category = line[:-5]\n",
        "    test_set.append(data_dir + category + '/' + line + '.txt')\n",
        "    if category not in category_names:\n",
        "      raise ValueError('Unknown category ' + category)\n",
        "    test_labels.append(category_names.index(category))\n",
        "\n",
        "# load training data\n",
        "train_data_points = np.empty([len(train_set), points_per_file, 3])\n",
        "\n",
        "print(f'### loading modelnet{num_classes} train ###')\n",
        "for i, filename in enumerate(train_set):\n",
        "  points, _ = \\\n",
        "      io.load_points_from_file_to_numpy(filename,\n",
        "                                        max_num_points=points_per_file)\n",
        "  train_data_points[i] = points\n",
        "  if i % 500 == 0:\n",
        "    print(f'{i}/{len(train_set)}')\n",
        "  if quick_test and i > 100:\n",
        "    break\n",
        "\n",
        "# load test data\n",
        "test_data_points = np.empty([len(test_set), points_per_file, 3])\n",
        "\n",
        "print(f'### loading modelnet{num_classes} test ###')\n",
        "for i, filename in enumerate(test_set):\n",
        "  points, _ = \\\n",
        "      io.load_points_from_file_to_numpy(filename,\n",
        "                                        max_num_points=points_per_file)\n",
        "  test_data_points[i] = points\n",
        "  if i % 500 == 0:\n",
        "    print(f'{i}/{len(test_set)}')\n",
        "  if quick_test and i > 100:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7a1wLiPxCb3",
        "colab_type": "text"
      },
      "source": [
        "Now let's define a small data loader.\n",
        "\n",
        "To make the network evaluation faster, we randomly samples the point clouds to reduce the input size.\n",
        "\n",
        "As we don't want to provide any additional features other than the point location to the network, we will use a constant `1` as input feature.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPJ9chpWxHNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class modelnet_data_generator(tf.keras.utils.Sequence):\n",
        "  ''' Small generator of batched data.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               points,\n",
        "               labels,\n",
        "               batch_size):\n",
        "      self.points = points\n",
        "      self.labels = np.array(labels, dtype=int)\n",
        "      self.batch_size = batch_size\n",
        "      self.epoch_size = len(self.points)\n",
        "\n",
        "      self.ids = np.arange(0, points_per_file)\n",
        "      # shuffle data before training\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    # number of batches per epoch\n",
        "    return(int(np.floor(self.epoch_size / self.batch_size)))\n",
        "\n",
        "  def __call__(self):\n",
        "    ''' Loads batch and increases batch index.\n",
        "    '''\n",
        "    data = self.__getitem__(self.index)\n",
        "    self.index += 1\n",
        "    return data\n",
        "\n",
        "  def __getitem__(self, index, samples_per_model=1024):\n",
        "    ''' Loads data of current batch and samples random subset of the points.\n",
        "    '''\n",
        "    labels = \\\n",
        "        self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "    points = \\\n",
        "        self.points[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "    # constant input feature\n",
        "    features = tf.ones([self.batch_size, samples_per_model, 1])\n",
        "\n",
        "    # sample points\n",
        "    sampled_points = np.empty([self.batch_size, samples_per_model, 3])\n",
        "    for batch in range(self.batch_size):\n",
        "      selection = np.random.choice(self.ids, samples_per_model)\n",
        "      sampled_points[batch] = points[batch][selection]\n",
        "\n",
        "    return sampled_points, features, labels\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    ''' Shuffles data and resets batch index.\n",
        "    '''\n",
        "    shuffle = np.random.permutation(np.arange(0, len(self.points)))\n",
        "    self.points = self.points[shuffle]\n",
        "    self.labels = self.labels[shuffle]\n",
        "    self.index = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txT-ddp_wTd0",
        "colab_type": "text"
      },
      "source": [
        "### Network architecture\n",
        "\n",
        "Let's build a simple classification network, which uses point convolutions for encoding the shape, and two dense layers for predicting the class.\n",
        "\n",
        "The following model contains example calls for the three different available point convolutions in the `layer` module:\n",
        "\n",
        "\n",
        "*   [Monte-Carlo convolutions](https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/viscom/2018/hermosilla2018montecarlo.pdf), which uses MLPs for representing the convolutional kernel, and aggregates the features inside the convolution radius using [Monte-Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), where each feature is weighted by a point density estimation.\n",
        "*   [Kernel Point convolutions](https://arxiv.org/pdf/1904.08889.pdf), where the convolutional kernel is represented by a set of weights on kernel points, which are interpolated. \n",
        "(Note: We use rigid kernel points in the example below but deformable kernel points are also supported)\n",
        "\n",
        "*   [PointConv convolutions](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.pdf)\n",
        ", which uses a single MLP for representing the convolutional kernel, and aggregates the features using an integration, where each feature is weighted by a learned inverse density estimation.\n",
        "\n",
        "\n",
        "Note that different to an image convolution layer, a point convolution layer needs additional input about the spatial location of the features, i.e. point coordinates.\n",
        "In case of a 'strided' point convolution, where the output features are defined on different points than the input, we have to provide two point clouds.\n",
        "\n",
        "For sampling the point clouds to lower densities, we use the `PointHierarchy` class.\n",
        "\n",
        "At the end of the encoder we use a `GlobalAveragePooling` layer to aggregate the features from all points into one latent vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGC3q1drt60i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylib.pc import layers\n",
        "\n",
        "class mymodel(tf.keras.Model):\n",
        "  ''' Model architecture with `L` convolutional layers followed by \n",
        "  two dense layers.\n",
        "\n",
        "  Args:\n",
        "    features_sizes: A `list` of `ints`, the feature dimensions. Shape `[L+3]`.\n",
        "    sample_radii: A `list` of `floats, the radii used for sampling\n",
        "      of the point clouds. Shape `[L]`.\n",
        "    conv_radii: A `list` of `floats`, the radii used by the convolution\n",
        "      layers. Shape `[L]`.\n",
        "    layer_type: A `string`, the type of convolution used,\n",
        "      can be 'MCConv', 'KPConv', 'PointConv'.\n",
        "    sampling_method: 'poisson disk' or 'cell average'.\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               feature_sizes,\n",
        "               sample_radii,\n",
        "               conv_radii,\n",
        "               layer_type='MCConv',\n",
        "               sampling_method='poisson disk'):\n",
        "    super(mymodel, self).__init__()\n",
        "    self.num_layers = len(sample_radii)\n",
        "    self.sample_radii = sample_radii.reshape(-1,1)\n",
        "    self.conv_radii = conv_radii\n",
        "    self.sampling_method = sampling_method\n",
        "    self.conv_layers = []\n",
        "    self.batch_layers = []\n",
        "    self.dense_layers = []\n",
        "    self.activations = []\n",
        "    # encoder\n",
        "    for i in range(self.num_layers):\n",
        "      # convolutional downsampling layers\n",
        "      if layer_type == 'MCConv':\n",
        "        self.conv_layers.append(layers.MCConv(\n",
        "            num_features_in=feature_sizes[i],\n",
        "            num_features_out=feature_sizes[i + 1],\n",
        "            num_dims=3,\n",
        "            num_mlps=4,\n",
        "            mlp_size=[8]))\n",
        "      elif layer_type == 'PointConv':\n",
        "        self.conv_layers.append(layers.PointConv(\n",
        "            num_features_in=feature_sizes[i],\n",
        "            num_features_out=feature_sizes[i + 1],\n",
        "            num_dims=3,\n",
        "            size_hidden=32))\n",
        "      elif layer_type == 'KPConv':\n",
        "        self.conv_layers.append(layers.KPConv(\n",
        "            num_features_in=feature_sizes[i],\n",
        "            num_features_out=feature_sizes[i + 1],\n",
        "            num_dims=3,\n",
        "            num_kernel_points=15))\n",
        "      else:\n",
        "        raise ValueError(\"Unknown layer type!\")\n",
        "      if i < self.num_layers-1:\n",
        "        # batch normalization and activation function\n",
        "        self.batch_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.activations.append(tf.keras.layers.LeakyReLU())\n",
        "    # global pooling\n",
        "    self.global_pooling = layers.GlobalAveragePooling()\n",
        "    self.batch_layers.append(tf.keras.layers.BatchNormalization())\n",
        "    self.activations.append(tf.keras.layers.LeakyReLU())\n",
        "    # MLP\n",
        "    self.dense_layers.append(tf.keras.layers.Dense(feature_sizes[-2]))\n",
        "    self.batch_layers.append(tf.keras.layers.BatchNormalization())\n",
        "    self.activations.append(tf.keras.layers.LeakyReLU())\n",
        "    self.dense_layers.append(tf.keras.layers.Dense(feature_sizes[-1]))\n",
        "\n",
        "  def __call__(self,\n",
        "               points,\n",
        "               features,\n",
        "               training):\n",
        "    ''' Evaluates network.\n",
        "\n",
        "    Args:\n",
        "      points: The point coordinates. Shape `[B, N, 3]`.\n",
        "      features: Input features. Shape `[B, N, C]`.\n",
        "      training: A `bool`, passed to the batch norm layers.\n",
        "\n",
        "    Returns:\n",
        "      The logits per class.\n",
        "    '''\n",
        "    sample_radii = self.sample_radii\n",
        "    conv_radii = self.conv_radii\n",
        "    sampling_method = self.sampling_method\n",
        "    # input point cloud\n",
        "    # Note: Here all point clouds have the same number of points, so no `sizes`\n",
        "    #       or `batch_ids` are passed.\n",
        "    point_cloud = pc.PointCloud(points)\n",
        "    # spatial downsampling\n",
        "    point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                        sample_radii,\n",
        "                                        sampling_method)\n",
        "    # network evaluation\n",
        "    for i in range(self.num_layers):\n",
        "      features = self.conv_layers[i](features,\n",
        "                                     point_hierarchy[i], \n",
        "                                     point_hierarchy[i+1],\n",
        "                                     conv_radii[i])\n",
        "      if i < self.num_layers-1:\n",
        "        features = self.batch_layers[i](features, training=training)\n",
        "        features = self.activations[i](features)\n",
        "    # classification head\n",
        "    features = self.global_pooling(features, point_hierarchy[-1])\n",
        "    features = self.batch_layers[-2](features, training)\n",
        "    features = self.activations[-2](features)\n",
        "    features = self.dense_layers[-2](features)\n",
        "    features = self.batch_layers[-1](features, training)\n",
        "    features = self.activations[-1](features)\n",
        "    return self.dense_layers[-1](features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WureY8ZixvlG",
        "colab_type": "text"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0pnDkbZxpxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "feature_sizes = [1, 128, 256, 512, 128, num_classes]\n",
        "sample_radii = np.array([0.1, 0.2, 0.4])\n",
        "conv_radii = sample_radii * 1.5\n",
        "\n",
        "# initialize data generators\n",
        "gen_train = modelnet_data_generator(train_data_points, train_labels, batch_size)\n",
        "gen_test = modelnet_data_generator(test_data_points, test_labels, batch_size)\n",
        "\n",
        "# loss function and optimizer\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "lr_decay=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01,\n",
        "    decay_steps=len(gen_train),\n",
        "    decay_rate=0.95)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJjR2m9Fx68-",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzolzewtyDcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model,\n",
        "             optimizer,\n",
        "             loss_function,\n",
        "             num_epochs = 10,\n",
        "             epochs_print=1):\n",
        "  train_loss_results = []\n",
        "  train_accuracy_results = []\n",
        "  test_loss_results = []\n",
        "  test_accuracy_results = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    time_epoch_start = time.time()\n",
        "\n",
        "    # --- Training ---\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for points, features, labels in gen_train:\n",
        "      # evaluate model; forward pass\n",
        "      with tf.GradientTape() as tape:\n",
        "        logits = model(points, features, training=True)\n",
        "        pred = tf.nn.softmax(logits, axis=-1)\n",
        "        loss = loss_function(y_true=labels, y_pred=pred)\n",
        "      # backpropagation\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      epoch_loss_avg.update_state(loss)\n",
        "      epoch_accuracy.update_state(labels, pred)\n",
        "\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    # --- Validation ---\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for points, features, labels in gen_test:\n",
        "      # evaluate model; forward pass\n",
        "      logits = model(points, features, training=False)\n",
        "      pred = tf.nn.softmax(logits, axis=-1)\n",
        "      loss = loss_function(y_true=labels, y_pred=pred)\n",
        "\n",
        "      epoch_loss_avg.update_state(loss)\n",
        "      epoch_accuracy.update_state(labels, pred)\n",
        "\n",
        "    test_loss_results.append(epoch_loss_avg.result())\n",
        "    test_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    time_epoch_end = time.time()\n",
        "\n",
        "    if epoch % epochs_print == 0:\n",
        "      # End epoch\n",
        "      print('Epoch {:03d} Time: {:.3f}s'.format(\n",
        "          epoch,\n",
        "          time_epoch_end - time_epoch_start))\n",
        "      print('Training:   Loss: {:.3f}, Accuracy: {:.3%}'.format(\n",
        "          train_loss_results[-1],\n",
        "          train_accuracy_results[-1]))\n",
        "      print('Validation: Loss: {:.3f}, Accuracy: {:.3%}'.format(\n",
        "          test_loss_results[-1],\n",
        "          test_accuracy_results[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIALtH8TzagI",
        "colab_type": "text"
      },
      "source": [
        "#### Train with [Monte-Carlo convolutions](https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/viscom/2018/hermosilla2018montecarlo.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-jbnyGaCrfE",
        "colab": {}
      },
      "source": [
        "model_MC = mymodel(feature_sizes, sample_radii, conv_radii,\n",
        "                   layer_type='MCConv')\n",
        "training(model_MC, optimizer, loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-6sJnRzXvl",
        "colab_type": "text"
      },
      "source": [
        "#### Train with [Kernel Point convolutions](https://arxiv.org/pdf/1904.08889.pdf).\n",
        "\n",
        "To use the cell average sampling used in the paper, we can simply change the sampling method, which is passed to the point hierarchy constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n03SZUTgzYoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_KP = mymodel(feature_sizes, sample_radii, conv_radii,\n",
        "                   layer_type='KPConv', sampling_method='cell average')\n",
        "training(model_KP, optimizer, loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak7hMQeWzZZq",
        "colab_type": "text"
      },
      "source": [
        "#### Train with [PointConv convolutions](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj9T-UaCzZ4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_PC = mymodel(feature_sizes, sample_radii, conv_radii,\n",
        "                   layer_type='PointConv')\n",
        "training(model_PC, optimizer, loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njP1uIRCjXJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}