{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFG_point_clouds_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2xKG5Pgu_SX",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RkUBOZbu3iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31faWJ7pCNGR"
      },
      "source": [
        "# Point Clouds for tensorflow_graphics\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/schellmi42/graphics/blob/point_convolutions/tensorflow_graphics/projects/point_convolutions/pylib/notebooks/Introduction.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/schellmi42/graphics/blob/point_convolutions/tensorflow_graphics/projects/point_convolutions/pylib/notebooks/Introduction.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo1bELqqRAKF",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqSeyzzZQUDV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Clone repositories, and install requirements and custom_op package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vtvcRP3QtTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone repositories\n",
        "!rm -r graphics\n",
        "!git clone https://github.com/schellmi42/graphics\n",
        "\n",
        "# install requirements and load tfg module \n",
        "!pip install -r graphics/requirements.txt\n",
        "\n",
        "# install custom ops\n",
        "!pip install graphics/tensorflow_graphics/projects/point_convolutions/custom_ops/pkg_builds/tf_2.2.0/*.whl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D-q8bUu7TcJ",
        "colab_type": "text"
      },
      "source": [
        "### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lTxNysx7Lfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# (this is equivalent to export PYTHONPATH='$HOME/graphics:/content/graphics:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics\")\n",
        "\n",
        "# load point cloud module \n",
        "# (this is equivalent to export PYTHONPATH='/content/graphics/tensorflow_graphics/projects/point_convolutions:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics/tensorflow_graphics/projects/point_convolutions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gUTDsKERxWh",
        "colab_type": "text"
      },
      "source": [
        "Check if it loads without errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_49mMLOSOX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_graphics as tfg\n",
        "import pylib.pc as pc\n",
        "import numpy as np\n",
        "\n",
        "print('TensorFlow version: %s'%tf.__version__)\n",
        "print('TensorFlow-Graphics version: %s'%tfg.__version__)\n",
        "print('Point Cloud Module: ', pc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4ryCVBRH4C",
        "colab_type": "text"
      },
      "source": [
        "## Example Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0hxQN7g-UH0",
        "colab_type": "text"
      },
      "source": [
        "### 2D square point clouds using segmentation IDs\n",
        "Here we create a batch of point clouds with variable number of points per cloud from unordered points with an additional id tensor.\n",
        "\n",
        "The `batch_ids` are the segmentation ids, which indicate which point belongs to which point cloud in the batch. For more information on segmentation IDs see: [tf.math#segmentation](https://www.tensorflow.org/api_docs/python/tf/math#Segmentation)\n",
        "\n",
        "If the points are ordered by batch id, it is also possible to pass a `sizes` tensor, which has the size of each point cloud in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWEkWPeJLoe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def square(num_samples, size=1):\n",
        "  # 2D square in 3D for easier visualization\n",
        "  points = np.random.rand(num_samples, 2)*2-1\n",
        "  return points*size\n",
        "\n",
        "num_samples=1000\n",
        "batch_size = 10\n",
        "\n",
        "# create numpy input data consisting of points and segmentation identifiers\n",
        "points = square(num_samples)\n",
        "batch_ids = np.random.randint(0, batch_size, num_samples)\n",
        "\n",
        "# create tensorflow point cloud\n",
        "point_cloud = pc.PointCloud(points, batch_ids, batch_size)\n",
        "\n",
        "# print information\n",
        "sizes = point_cloud.get_sizes()\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "print(sizes.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qStvELcmgd7R",
        "colab_type": "text"
      },
      "source": [
        "Create a batch of point hierarchies using sequential poisson disk sampling with pooling radii 0.1, 0.4, 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6A9YdQ8IAIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy input parameters\n",
        "sampling_radii = np.array([[0.1], [0.4], [2]])\n",
        "\n",
        "# create tensorflow point hierarchy\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    sampling_radii,\n",
        "                                    'poisson_disk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRMSzdQaInJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print information\n",
        "num_levels = len(sampling_radii) + 1\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for i in range(num_levels):\n",
        "  print('level: ' + str(i))\n",
        "  print(sizes[i].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnDmsPWiEqR",
        "colab_type": "text"
      },
      "source": [
        "assign a shape to the batch and look at the sizes again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDNRdJayiKfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "point_hierarchy.set_batch_shape([2, 5])\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for i in range(num_levels):\n",
        "  print('level: ' + str(i))\n",
        "  print(sizes[i].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWlLEYXmwtmn",
        "colab_type": "text"
      },
      "source": [
        "Visualize the levels of one example from the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq103KhWbHYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# which example from the batch to choose, can be 'int' or relative in [A1,...,An]\n",
        "batch_id = [0,1]\n",
        "\n",
        "curr_points = point_hierarchy.get_points(batch_id)\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize=[num_levels*5,5])\n",
        "for i in range(num_levels):\n",
        "  plt.subplot(1,num_levels,i+1)\n",
        "  plt.plot(curr_points[i][:, 0],curr_points[i][:, 1],'bo')\n",
        "  plt.axis([-1, 1, -1, 1])\n",
        "  if i==0:\n",
        "    plt.title('input point cloud')\n",
        "  else:\n",
        "    plt.title('poisson sampled points with radius %s'%sampling_radii[i - 1, 0])\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZnSGv7cnbc",
        "colab_type": "text"
      },
      "source": [
        "### 3D point clouds from input files using arbitrary batch sizes with padding\n",
        "\n",
        "Here we create point clouds from input files using a zero padded representation of shape `[A1, .., An, V, D]`.\n",
        "Internally this is converted to a segmented representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Sd7egJlsbQ",
        "colab_type": "text"
      },
      "source": [
        " #### Loading from ASCII .txt files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wK7aZNVcwwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pylib.io as io\n",
        "\n",
        "# SHREC15\n",
        "\n",
        "#### get files ####\n",
        "input_dir = 'graphics/tensorflow_graphics/projects/point_convolutions/test_point_clouds/SHREC15/'\n",
        "filenames = tf.io.gfile.listdir(input_dir)\n",
        "batch_size = len(filenames)\n",
        "print('### batch size ###'); print(batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  filenames[i] = input_dir + filenames[i]\n",
        "\n",
        "#### load points #####\n",
        "batch_shape = [5,2]\n",
        "print('### batch shape###'); print(batch_shape)\n",
        "points, normals, sizes = io.load_batch_of_points(filenames, batch_shape = batch_shape)\n",
        "\n",
        "print('### data shape ###'); print(points.shape)\n",
        "print('### points per point cloud ###');print(sizes.numpy())\n",
        "\n",
        "#### build point hierarchy #####\n",
        "point_cloud = pc.PointCloud(points, sizes=sizes)\n",
        "\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    [[0.05], [0.1]],\n",
        "                                    'poisson_disk')\n",
        "\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "\n",
        "print('### point per point cloud in hierarchy ###')\n",
        "for level in range(len(sizes)):\n",
        "  print('level %s'%level)\n",
        "  print(sizes[level].numpy())\n",
        "\n",
        "### extract points from last level in original batch shape ###\n",
        "hierarchical_points = point_hierarchy.get_points()\n",
        "out_points = hierarchical_points[-1]\n",
        "print('### shape of points in last level ###'); print(out_points.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pjPEofkgvC6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading vertices from mesh files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umdcLRWhgzjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thingi10k meshes\n",
        "\n",
        "#### get files ####\n",
        "input_dir = 'graphics/tensorflow_graphics/projects/point_convolutions/test_point_clouds/meshes/'\n",
        "filenames = tf.io.gfile.listdir(input_dir)\n",
        "batch_size = len(filenames)\n",
        "print('### batch size ###'); print(batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  filenames[i] = input_dir+filenames[i]\n",
        "\n",
        "#### load points ####\n",
        "points, sizes = io.load_batch_of_meshes(filenames)\n",
        "\n",
        "print('### data shape ###'); print(points.shape)\n",
        "print('### points per point cloud ###');print(sizes.numpy())\n",
        "\n",
        "#### build a point cloud object ####\n",
        "point_cloud = pc.PointCloud(points, sizes=sizes)\n",
        "\n",
        "print('### internal shape conversion ###')\n",
        "print('Input    (padded):    %s elements'%len(tf.reshape(points, [-1, 3])))\n",
        "print('Internal (segmented): %s elements'%len(point_cloud._points))\n",
        "\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    [[0.05], [0.1]],\n",
        "                                    'poisson_disk')\n",
        "\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "\n",
        "print('### point per point cloud in hierarchy ###')\n",
        "for level in range(len(sizes)):\n",
        "  print('level %s'%level)\n",
        "  print(sizes[level].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMwvIHLhnt6m",
        "colab_type": "text"
      },
      "source": [
        "### Monte-Carlo Convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRDxnD1in0jH",
        "colab_type": "text"
      },
      "source": [
        "Create convolutions for a point hierarchy with MLPs as kernel \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuSf3jxwjVox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "### create random input data\n",
        "num_pts = 1000\n",
        "point_dim = 3\n",
        "feature_dim = 3\n",
        "batch_size = 10\n",
        "\n",
        "# create random points\n",
        "points = np.random.rand(num_pts,point_dim)\n",
        "batch_ids = np.random.randint(0,batch_size,num_pts)\n",
        "batch_ids[:batch_size] = np.arange(0,batch_size) # ensure non-empty point clouds\n",
        "# create random features\n",
        "features = np.random.rand(num_pts,feature_dim)\n",
        "\n",
        "# build initial point cloud\n",
        "point_cloud = pc.PointCloud(points, batch_ids, batch_size)\n",
        "\n",
        "# build point hierarchy\n",
        "sample_radii = np.array([[0.1],[0.2],[2]])\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,sample_radii)\n",
        "\n",
        "### build model\n",
        "\n",
        "# layer parameters\n",
        "conv_radii = 2*sample_radii\n",
        "feature_sizes = [8,16,32]\n",
        "kernel_hidden_size = 8 # number of neurons in the hidden layer of the kernel MLP\n",
        "\n",
        "### initialize layers\n",
        "Conv1 = pc.layers.MCConv(feature_dim, feature_sizes[0], point_dim,kernel_hidden_size)\n",
        "Conv2 = pc.layers.MCConv(feature_sizes[0],feature_sizes[1],point_dim,kernel_hidden_size)\n",
        "Conv3 = pc.layers.MCConv(feature_sizes[1],feature_sizes[2],point_dim,kernel_hidden_size)\n",
        "\n",
        "### call layers\n",
        "conv1_result = Conv1(features,point_hierarchy[0], point_hierarchy[1],conv_radii[0])\n",
        "conv2_result = Conv2(conv1_result,point_hierarchy[1], point_hierarchy[2],conv_radii[1])\n",
        "conv3_result = Conv3(conv2_result,point_hierarchy[2], point_hierarchy[3],conv_radii[2], return_sorted=True)\n",
        "\n",
        "### printing \n",
        "print('### point cloud sizes ###')\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for s in sizes:\n",
        "  print(s.numpy())\n",
        "\n",
        "print('\\n### features dimensions flat ###')\n",
        "print('Input: ');print(features.shape)\n",
        "print('Conv1: ');print(conv1_result.shape)\n",
        "print('Conv2: ');print(conv2_result.shape)\n",
        "print('Conv3: ');print(conv3_result.shape)\n",
        "\n",
        "# again in padded format\n",
        "point_hierarchy.set_batch_shape([5,2])\n",
        "\n",
        "unflatten = point_hierarchy[0].get_unflatten()\n",
        "features_padded = unflatten(features)\n",
        "### call layers\n",
        "conv1_result_padded = Conv1(features_padded, point_hierarchy[0], point_hierarchy[1],conv_radii[0], return_padded=True)\n",
        "conv2_result_padded = Conv2(conv1_result_padded, point_hierarchy[1], point_hierarchy[2],conv_radii[1], return_padded=True)\n",
        "conv3_result_padded = Conv3(conv2_result_padded, point_hierarchy[2], point_hierarchy[3],conv_radii[2], return_padded=True)\n",
        "print('\\n### feature dimensions padded ###')\n",
        "print('Input: ');print(features_padded.shape)\n",
        "print('Conv1: ');print(conv1_result_padded.shape)\n",
        "print('Conv2: ');print(conv2_result_padded.shape)\n",
        "print('Conv3: ');print(conv3_result_padded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}