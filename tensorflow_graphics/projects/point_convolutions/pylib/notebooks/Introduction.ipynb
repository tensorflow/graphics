{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFG_point_clouds_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2xKG5Pgu_SX",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RkUBOZbu3iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31faWJ7pCNGR"
      },
      "source": [
        "# Point Clouds for tensorflow_graphics\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/schellmi42/tensorflow_graphics_point_clouds/blob/master/pylib/notebooks/Introduction.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/schellmi42/tensorflow_graphics_point_clouds/blob/master/pylib/notebooks/Introduction.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo1bELqqRAKF",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqSeyzzZQUDV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Clone repositories, and install requirements and custom_op package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vtvcRP3QtTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone repositories\n",
        "!rm -r graphics\n",
        "!git clone https://github.com/schellmi42/graphics\n",
        "\n",
        "# install requirements and load tfg module \n",
        "!pip install -r graphics/requirements.txt\n",
        "\n",
        "# install custom ops\n",
        "!pip install graphics/tensorflow_graphics/projects/point_convolutions/custom_ops/pkg_builds/tf_2.2.0/*.whl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D-q8bUu7TcJ",
        "colab_type": "text"
      },
      "source": [
        "### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lTxNysx7Lfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# (this is equivalent to export PYTHONPATH='$HOME/graphics:/content/graphics:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics\")\n",
        "\n",
        "# load point cloud module \n",
        "# (this is equivalent to export PYTHONPATH='/content/graphics/tensorflow_graphics/projects/point_convolutions:$PYTHONPATH', but adds path to running session)\n",
        "sys.path.append(\"/content/graphics/tensorflow_graphics/projects/point_convolutions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gUTDsKERxWh",
        "colab_type": "text"
      },
      "source": [
        "Check if it loads without errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_49mMLOSOX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_graphics as tfg\n",
        "import pylib.pc as pc\n",
        "import numpy as np\n",
        "\n",
        "print('TensorFlow version: %s'%tf.__version__)\n",
        "print('TensorFlow-Graphics version: %s'%tfg.__version__)\n",
        "print('Point Cloud Module: ', pc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4ryCVBRH4C",
        "colab_type": "text"
      },
      "source": [
        "## Example Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0hxQN7g-UH0",
        "colab_type": "text"
      },
      "source": [
        "### 2D square point clouds using segmentation IDs\n",
        "Here we create a batch of point clouds with variable number of points per cloud from unordered points with an additional id tensor.\n",
        "\n",
        "The `batch_ids` are the segmentation ids, which indicate which point belongs to which point cloud in the batch. For more information on segmentation IDs see: [tf.math#segmentation](https://www.tensorflow.org/api_docs/python/tf/math#Segmentation)\n",
        "\n",
        "If the points are ordered by batch id, it is also possible to pass a `sizes` tensor, which has the size of each point cloud in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWEkWPeJLoe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def square(num_samples, size=1):\n",
        "  # 2D square in 3D for easier visualization\n",
        "  points = np.random.rand(num_samples, 2)*2-1\n",
        "  return points*size\n",
        "\n",
        "num_samples=1000\n",
        "batch_size = 10\n",
        "\n",
        "# create numpy input data consisting of points and segmentation identifiers\n",
        "points = square(num_samples)\n",
        "batch_ids = np.random.randint(0, batch_size, num_samples)\n",
        "\n",
        "# create tensorflow point cloud\n",
        "point_cloud = pc.PointCloud(points, batch_ids, batch_size)\n",
        "\n",
        "# print information\n",
        "sizes = point_cloud.get_sizes()\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "print(sizes.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qStvELcmgd7R",
        "colab_type": "text"
      },
      "source": [
        "Create a batch of point hierarchies using sequential poisson disk sampling with pooling radii 0.1, 0.4, 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6A9YdQ8IAIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy input parameters\n",
        "sampling_radii = np.array([[0.1], [0.4], [2]])\n",
        "\n",
        "# create tensorflow point hierarchy\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    sampling_radii,\n",
        "                                    'poisson_disk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRMSzdQaInJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print information\n",
        "num_levels = len(sampling_radii) + 1\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for i in range(num_levels):\n",
        "  print('level: ' + str(i))\n",
        "  print(sizes[i].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnDmsPWiEqR",
        "colab_type": "text"
      },
      "source": [
        "assign a shape to the batch and look at the sizes again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDNRdJayiKfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "point_hierarchy.set_batch_shape([2, 5])\n",
        "print('%s point clouds of sizes:'%point_cloud._batch_size)\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for i in range(num_levels):\n",
        "  print('level: ' + str(i))\n",
        "  print(sizes[i].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWlLEYXmwtmn",
        "colab_type": "text"
      },
      "source": [
        "Visualize the levels of one example from the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq103KhWbHYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# which example from the batch to choose, can be 'int' or relative in [A1,...,An]\n",
        "batch_id = [0,1]\n",
        "\n",
        "curr_points = point_hierarchy.get_points(batch_id)\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize=[num_levels*5,5])\n",
        "for i in range(num_levels):\n",
        "  plt.subplot(1,num_levels,i+1)\n",
        "  plt.plot(curr_points[i][:, 0],curr_points[i][:, 1],'bo')\n",
        "  plt.axis([-1, 1, -1, 1])\n",
        "  if i==0:\n",
        "    plt.title('input point cloud')\n",
        "  else:\n",
        "    plt.title('poisson sampled points with radius %s'%sampling_radii[i - 1, 0])\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZnSGv7cnbc",
        "colab_type": "text"
      },
      "source": [
        "### 3D point clouds from input files using arbitrary batch sizes with padding\n",
        "\n",
        "Here we create point clouds from input files using a zero padded representation of shape `[A1, .., An, V, D]`.\n",
        "Internally this is converted to a segmented representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Sd7egJlsbQ",
        "colab_type": "text"
      },
      "source": [
        " #### Loading from ASCII .txt files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wK7aZNVcwwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pylib.io as io\n",
        "\n",
        "# SHREC15\n",
        "\n",
        "#### get files ####\n",
        "input_dir = 'graphics/tensorflow_graphics/projects/point_convolutions/test_point_clouds/SHREC15/'\n",
        "filenames = tf.io.gfile.listdir(input_dir)\n",
        "batch_size = len(filenames)\n",
        "print('### batch size ###'); print(batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  filenames[i] = input_dir + filenames[i]\n",
        "\n",
        "#### load points #####\n",
        "batch_shape = [5,2]\n",
        "print('### batch shape###'); print(batch_shape)\n",
        "points, normals, sizes = io.load_batch_of_points(filenames, batch_shape = batch_shape)\n",
        "\n",
        "print('### data shape ###'); print(points.shape)\n",
        "print('### points per point cloud ###');print(sizes.numpy())\n",
        "\n",
        "#### build point hierarchy #####\n",
        "point_cloud = pc.PointCloud(points, sizes=sizes)\n",
        "\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    [[0.05], [0.1]],\n",
        "                                    'poisson_disk')\n",
        "\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "\n",
        "print('### point per point cloud in hierarchy ###')\n",
        "for level in range(len(sizes)):\n",
        "  print('level %s'%level)\n",
        "  print(sizes[level].numpy())\n",
        "\n",
        "### extract points from last level in original batch shape ###\n",
        "hierarchical_points = point_hierarchy.get_points()\n",
        "out_points = hierarchical_points[-1]\n",
        "print('### shape of points in last level ###'); print(out_points.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pjPEofkgvC6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading vertices from mesh files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umdcLRWhgzjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thingi10k meshes\n",
        "\n",
        "#### get files ####\n",
        "input_dir = 'graphics/tensorflow_graphics/projects/point_convolutions/test_point_clouds/meshes/'\n",
        "filenames = tf.io.gfile.listdir(input_dir)\n",
        "batch_size = len(filenames)\n",
        "print('### batch size ###'); print(batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  filenames[i] = input_dir+filenames[i]\n",
        "\n",
        "#### load points ####\n",
        "points, sizes = io.load_batch_of_meshes(filenames)\n",
        "\n",
        "print('### data shape ###'); print(points.shape)\n",
        "print('### points per point cloud ###');print(sizes.numpy())\n",
        "\n",
        "#### build a point cloud object ####\n",
        "point_cloud = pc.PointCloud(points, sizes=sizes)\n",
        "\n",
        "print('### internal shape conversion ###')\n",
        "print('Input    (padded):    %s elements'%len(tf.reshape(points, [-1, 3])))\n",
        "print('Internal (segmented): %s elements'%len(point_cloud._points))\n",
        "\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,\n",
        "                                    [[0.05], [0.1]],\n",
        "                                    'poisson_disk')\n",
        "\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "\n",
        "print('### point per point cloud in hierarchy ###')\n",
        "for level in range(len(sizes)):\n",
        "  print('level %s'%level)\n",
        "  print(sizes[level].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMwvIHLhnt6m",
        "colab_type": "text"
      },
      "source": [
        "### Monte-Carlo Convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRDxnD1in0jH",
        "colab_type": "text"
      },
      "source": [
        "Create convolutions for a point hierarchy with MLPs as kernel \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuSf3jxwjVox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "### create random input data\n",
        "num_pts = 1000\n",
        "point_dim = 3\n",
        "feature_dim = 3\n",
        "batch_size = 10\n",
        "\n",
        "# create random points\n",
        "points = np.random.rand(num_pts,point_dim)\n",
        "batch_ids = np.random.randint(0,batch_size,num_pts)\n",
        "batch_ids[:batch_size] = np.arange(0,batch_size) # ensure non-empty point clouds\n",
        "# create random features\n",
        "features = np.random.rand(num_pts,feature_dim)\n",
        "\n",
        "# build initial point cloud\n",
        "point_cloud = pc.PointCloud(points, batch_ids, batch_size)\n",
        "\n",
        "# build point hierarchy\n",
        "sample_radii = np.array([[0.1],[0.2],[2]])\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,sample_radii)\n",
        "\n",
        "### build model\n",
        "\n",
        "# layer parameters\n",
        "conv_radii = 2*sample_radii\n",
        "feature_sizes = [8,16,32]\n",
        "kernel_hidden_size = 8 # number of neurons in the hidden layer of the kernel MLP\n",
        "\n",
        "### initialize layers\n",
        "Conv1 = pc.layers.MCConv(feature_dim, feature_sizes[0], point_dim,kernel_hidden_size)\n",
        "Conv2 = pc.layers.MCConv(feature_sizes[0],feature_sizes[1],point_dim,kernel_hidden_size)\n",
        "Conv3 = pc.layers.MCConv(feature_sizes[1],feature_sizes[2],point_dim,kernel_hidden_size)\n",
        "\n",
        "### call layers\n",
        "conv1_result = Conv1(features,point_hierarchy[0], point_hierarchy[1],conv_radii[0])\n",
        "conv2_result = Conv2(conv1_result,point_hierarchy[1], point_hierarchy[2],conv_radii[1])\n",
        "conv3_result = Conv3(conv2_result,point_hierarchy[2], point_hierarchy[3],conv_radii[2], return_sorted=True)\n",
        "\n",
        "### printing \n",
        "print('### point cloud sizes ###')\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for s in sizes:\n",
        "  print(s.numpy())\n",
        "\n",
        "print('\\n### features dimensions flat ###')\n",
        "print('Input: ');print(features.shape)\n",
        "print('Conv1: ');print(conv1_result.shape)\n",
        "print('Conv2: ');print(conv2_result.shape)\n",
        "print('Conv3: ');print(conv3_result.shape)\n",
        "\n",
        "# again in padded format\n",
        "point_hierarchy.set_batch_shape([5,2])\n",
        "\n",
        "unflatten = point_hierarchy[0].get_unflatten()\n",
        "features_padded = unflatten(features)\n",
        "### call layers\n",
        "conv1_result_padded = Conv1(features_padded, point_hierarchy[0], point_hierarchy[1],conv_radii[0], return_padded=True)\n",
        "conv2_result_padded = Conv2(conv1_result_padded, point_hierarchy[1], point_hierarchy[2],conv_radii[1], return_padded=True)\n",
        "conv3_result_padded = Conv3(conv2_result_padded, point_hierarchy[2], point_hierarchy[3],conv_radii[2], return_padded=True)\n",
        "print('\\n### feature dimensions padded ###')\n",
        "print('Input: ');print(features_padded.shape)\n",
        "print('Conv1: ');print(conv1_result_padded.shape)\n",
        "print('Conv2: ');print(conv2_result_padded.shape)\n",
        "print('Conv3: ');print(conv3_result_padded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUu8QpmNmosR",
        "colab_type": "text"
      },
      "source": [
        "### ResNet blocks and pooling layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPQ_skO6kGyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "### create random input data\n",
        "num_pts = 1000\n",
        "point_dim = 3\n",
        "feature_dim = 3\n",
        "batch_size = 10\n",
        "batch_shape = [5, 2]\n",
        "\n",
        "# create random points\n",
        "points = np.random.rand(num_pts, point_dim)\n",
        "batch_ids = np.random.randint(0, batch_size,num_pts)\n",
        "batch_ids[:batch_size] = np.arange(0, batch_size) # ensure non-empty point clouds\n",
        "# create random features\n",
        "features = np.random.rand(num_pts,feature_dim)\n",
        "\n",
        "# build initial point cloud\n",
        "point_cloud = pc.PointCloud(points, batch_ids, batch_size)\n",
        "\n",
        "# build point hierarchy\n",
        "sample_radii = np.array([[0.1]])\n",
        "point_hierarchy = pc.PointHierarchy(point_cloud,sample_radii)\n",
        "\n",
        "### build model\n",
        "\n",
        "# layer parameters\n",
        "conv_radii = np.array([0.2, 0.5])\n",
        "feature_sizes = [8]\n",
        "num_resnet_blocks =4 # number of ResNet blocks, each block has 2 layers\n",
        "layer_type = 'MCConv'\n",
        "\n",
        "### initialize layers\n",
        "Conv = pc.layers.MCConv(feature_dim, feature_sizes[0], point_dim, kernel_hidden_size)\n",
        "ResNet = pc.layers.PointResNet(feature_sizes[0], num_resnet_blocks, point_dim, layer_type)\n",
        "Pool = pc.layers.GlobalMaxPooling()\n",
        "\n",
        "### call layers\n",
        "conv_result = Conv(features, point_hierarchy[0], point_hierarchy[1], conv_radii[0])\n",
        "resnet_result = ResNet(conv_result, point_hierarchy[1], conv_radii[1], training=True)\n",
        "pool_result = Pool(resnet_result, point_hierarchy[1])\n",
        "\n",
        "### printing \n",
        "print('### point cloud sizes ###')\n",
        "sizes = point_hierarchy.get_sizes()\n",
        "for s in sizes:\n",
        "  print(s.numpy())\n",
        "\n",
        "print('\\n### feature dimensions flat ###')\n",
        "print('Input: ');print(features.shape)\n",
        "print('Conv: ');print(conv_result.shape)\n",
        "print('ResNet: ');print(resnet_result.shape)\n",
        "print('GlobalPool: ');print(pool_result.shape)\n",
        "\n",
        "# again in padded format\n",
        "\n",
        "point_hierarchy.set_batch_shape(batch_shape)\n",
        "unflatten = point_hierarchy[0].get_unflatten()\n",
        "features_padded = unflatten(features)\n",
        "### call layers\n",
        "conv_result_padded = Conv1(features_padded,point_hierarchy[0], point_hierarchy[1],conv_radii[0], return_padded=True)\n",
        "resnet_result_padded = ResNet(conv_result_padded, point_hierarchy[1], conv_radii[1], training=True, return_padded=True)\n",
        "pool_result_padded = Pool(resnet_result_padded, point_hierarchy[1], return_padded=True)\n",
        "print('\\n### feature dimensions padded ###')\n",
        "print('Input: ');print(features_padded.shape)\n",
        "print('Conv: ');print(conv_result_padded.shape)\n",
        "print('ResNet: ');print(resnet_result_padded.shape)\n",
        "print('GlobalPool: ');print(pool_result_padded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVT9mRhrBTH",
        "colab_type": "text"
      },
      "source": [
        "### Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM7o2JtO4WGz",
        "colab_type": "text"
      },
      "source": [
        "#### Reuse precomputed data structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrLIU7Md67wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import pylib.pc as pc\n",
        "import numpy as np\n",
        "# method to create a random point cloud\n",
        "from pylib.pc.tests.utils import _create_random_point_cloud_segmented\n",
        "\n",
        "# module for timing\n",
        "import time\n",
        "\n",
        "\n",
        "class conv_model():\n",
        "  \"\"\" A Monte-Carlo convolutional neural network (MCCNN) without downsampling,\n",
        "  just repeated convolutions\n",
        "  \"\"\"\n",
        "  def __init__(self, num_features, depth):\n",
        "    \"\"\" Constructor.\n",
        "\n",
        "    Args:\n",
        "      num_features: `int`, the dimensionality of the features.\n",
        "      depth: `int` the number of layers.\n",
        "    \"\"\"\n",
        "    self.depth = depth\n",
        "    self.layers = [pc.layers.PointConv(num_features_in=num_features,\n",
        "                                       num_features_out=num_features,\n",
        "                                       num_dims=3,\n",
        "                                       size_hidden=8)\n",
        "                   for i in range(depth)]\n",
        "  \n",
        "  def __call__(self, point_cloud, features, radius, neighborhood=None):\n",
        "    \"\"\" Evaluates the network.\n",
        "\n",
        "    Args:\n",
        "      point_cloud: A `PointCloud` instance, with N points.\n",
        "      features: A `float` tensor of shape [N,C], where C is `num_features`\n",
        "        from constructor.\n",
        "      radius: An `float`, the radius of the convolution.\n",
        "      neighborhood: A `Neighborhood` instance for `point_cloud` with cell_size radius, \n",
        "        if `None` a neighborhood is computed internally.\n",
        "\n",
        "      Returns:\n",
        "        A tensor of shape [N,C], the result of the convolutions.\n",
        "\n",
        "    \"\"\"\n",
        "    for layer in self.layers:\n",
        "      features = layer(features, point_cloud, point_cloud, radius=radius, neighborhood=neighborhood)\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def time_networks(num_points, radius):\n",
        "  if num_points >= 1e6:\n",
        "    print('\\n### number of points %sM ###'%(num_points//1000000))\n",
        "  else:\n",
        "    print('\\n### number of points %sk ###'%(num_points//1000))\n",
        "    \n",
        "  # hyper parameters\n",
        "  batch_size = 32\n",
        "  num_features = 8\n",
        "  depth = 10\n",
        "\n",
        "  # initialize the network \n",
        "  conv = conv_model(num_features, depth)\n",
        "\n",
        "  # create random input\n",
        "  points, batch_ids = _create_random_point_cloud_segmented(batch_size, num_points, dimension=3)\n",
        "  features = np.random.rand(num_points, num_features)\n",
        "  point_cloud = pc.PointCloud(points, batch_ids)\n",
        "\n",
        "  #### execute without precomupted neighborhood\n",
        "  t1 = time.time()\n",
        "  conv_result_1 = MCCNN(point_cloud, features, radius)\n",
        "  t2 = time.time()\n",
        "\n",
        "  print('recompute neighbors: %.3fs'%(t2-t1))\n",
        "\n",
        "  # precompute neighborhood\n",
        "  cell_sizes = [radius, radius, radius]\n",
        "  grid = pc.Grid(point_cloud, cell_sizes)\n",
        "  neighborhood = pc.Neighborhood(grid, cell_sizes)\n",
        "\n",
        "  # execute with precomputed neighborhood\n",
        "  t1 = time.time()\n",
        "  conv_result_2 = conv(point_cloud, features, radius, neighborhood)\n",
        "  t2 = time.time()\n",
        "  print('reuse neighbors:     %.3fs'%(t2-t1))\n",
        "\n",
        "time_networks(10000, 0.1)\n",
        "time_networks(100000, 0.1)\n",
        "time_networks(1000000, 0.01)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmCB8SPdOIs3",
        "colab_type": "text"
      },
      "source": [
        "#### Transpose Neighborhoods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FMdV4oH66gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylib.pc.tests.utils import _create_random_point_cloud_segmented\n",
        "from pylib.pc import PointCloud, Grid, Neighborhood\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def time_transpose_neighborhood(num_points, batch_size=8, num_reps=10):\n",
        "  if num_points >= 1e6:\n",
        "    print('\\n### number of points %sM ###'%(num_points//1000000))\n",
        "  else:\n",
        "    print('\\n### number of points %sk ###'%(num_points//1000))\n",
        "  num_poins = num_points\n",
        "  num_centers = max(num_points // 100, batch_size)\n",
        "  radius = 0.1\n",
        "  \n",
        "  nbs = []\n",
        "  point_clouds = []\n",
        "  point_clouds_centers = []\n",
        "  for i in range(num_reps):\n",
        "    points, batch_ids = _create_random_point_cloud_segmented(batch_size, num_points)\n",
        "    point_cloud = PointCloud(points, batch_ids)\n",
        "    points_centers, batch_ids_centers = _create_random_point_cloud_segmented(batch_size, num_centers)\n",
        "    point_cloud_centers = PointCloud(points_centers, batch_ids_centers)\n",
        "    grid = Grid(point_cloud, radius)\n",
        "\n",
        "    point_clouds.append(point_cloud)\n",
        "    point_clouds_centers.append(point_cloud_centers)\n",
        "    nbs.append(Neighborhood(grid, radius, point_cloud_centers))\n",
        "\n",
        "  t1 = time.time()\n",
        "  for i in range(num_reps):\n",
        "    grid_centers = Grid(point_clouds_centers[i], radius)\n",
        "    nb_t1 = Neighborhood(grid_centers, radius, point_clouds[i])\n",
        "  t2 = time.time()\n",
        "  t = 1000*(t2-t1)/num_reps\n",
        "  print('compute new neighborhood:        %.1fms'%(t))\n",
        "\n",
        "  t3 = time.time()\n",
        "  for i in range(num_reps):\n",
        "    nb_t2 = nbs[i].transpose()\n",
        "  t4 = time.time()\n",
        "  t = 1000*(t4-t3)/num_reps\n",
        "  print('transpose existing neighborhood: %.1fms'%(t))\n",
        "\n",
        "time_transpose_neighborhood(1000, num_reps=100)\n",
        "time_transpose_neighborhood(10000, num_reps=100)\n",
        "time_transpose_neighborhood(100000, num_reps=10)\n",
        "time_transpose_neighborhood(1000000, num_reps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsy9koKT8NS",
        "colab_type": "text"
      },
      "source": [
        "#### 1x1 Convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi2jKASzkhkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import plib.pc as pc\n",
        "import numpy as np\n",
        "# timing utilities\n",
        "import time\n",
        "# function to create a random point cloud\n",
        "from plib.pc.tests.utils import _create_random_point_cloud_segmented\n",
        "# custom gpu and tensorflow implementation of compute_pdf\n",
        "from pylib.pc.layers import Conv1x1, MCConv\n",
        "\n",
        "\n",
        "def time_1x1_conv(num_points,\n",
        "                  batch_size,\n",
        "                  num_feat,\n",
        "                  radius,\n",
        "                  hidden_size,\n",
        "                  dimension):\n",
        "  # create random point cloud\n",
        "  points, batch_ids = _create_random_point_cloud_segmented(\n",
        "      batch_size, num_points, dimension=dimension)\n",
        "  point_cloud = pc.PointCloud(points, batch_ids)\n",
        "  features = np.random.rand(num_points, num_feat[0])\n",
        "  # build conv_layers\n",
        "  conv1x1 = Conv1x1(num_feat[0], num_feat[1])\n",
        "  mcconv = MCConv(num_feat[0], num_feat[1], dimension, hidden_size)\n",
        "  # build 1x1 neighborhood\n",
        "  cell_sizes = np.float32(np.repeat(radius, dimension))\n",
        "  grid = pc.Grid(point_cloud, cell_sizes)\n",
        "  neighborhood = pc.Neighborhood(grid, radius)\n",
        "  # is1x1 = neighborhood._neighbors.shape[0] == num_points\n",
        "  # print('\\nMCConv is 1x1: ', is1x1)\n",
        "    \n",
        "  # compute conv\n",
        "  t1 = time.time()\n",
        "  neighbors  = tf.stack((tf.range(0, num_points), tf.range(0, num_points)), axis=1)\n",
        "  _ = mcconv(features, point_cloud, point_cloud, radius, neighborhood)\n",
        "  t2 = time.time()\n",
        "  _ = conv1x1(features, point_cloud)\n",
        "  t3 = time.time()\n",
        "\n",
        "  # printing\n",
        "  if num_points >= 1e6: \n",
        "    print('num points: %sM, feature dim: %s -> %s'%(num_points//int(1e6), num_feat[0], num_feat[1]))\n",
        "  elif num_points >= 1e3:\n",
        "    print('num points: %sk, feature dim: %s -> %s'%(num_points//int(1e3), num_feat[0], num_feat[1]))\n",
        "  else:\n",
        "    print('\\nnum points: %s'%num_points)\n",
        "  \n",
        "  num_p_mc = (1 + num_feat[0]) * hidden_size + hidden_size * num_feat[0] * num_feat[1]\n",
        "  num_p_1x1 = num_feat[0] * num_feat[1]\n",
        "  print('num params: %10d, time MCCNN:       %.4fs'%(num_p_mc, t2-t1))\n",
        "  print('num params: %10d, time Conv1x1:     %.4fs\\n'%(num_p_1x1, t3-t2))\n",
        "\n",
        "\n",
        "\n",
        "time_1x1_conv(10000, 32, [1, 1], 1e-3, 8, 3)\n",
        "time_1x1_conv(10000, 32, [64, 64], 1e-3, 8, 3)\n",
        "time_1x1_conv(10000, 32, [1, 2048], 1e-3, 8, 3)\n",
        "time_1x1_conv(10000, 32, [2048, 1], 1e-3, 8, 3)\n",
        "time_1x1_conv(10000, 32, [2048, 2048], 1e-3, 8, 3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}